# Detection of DoH using machine learning algorithms
Domain Name System (DNS) is one of the early and vulnerable network protocols which has several security loopholes that have been exploited repeatedly over the years. DNS abuse has always been an area of great concern for cybersecurity researchers. However, providing security and privacy to DNS requests and responses is still a challenging task as attackers use sophisticated attack methodologies to steal data on the fly.
To overcome some of the DNS vulnerabilities related to privacy and data manipulation, IETF introduced DNS over HTTPS (DoH) in RFC8484, a protocol that enhances privacy and combats eavesdropping and man-in-the-middle attacks by encrypting DNS queries and sending them in a covert channel/tunnel so that data is not hampered on the way. Nonetheless, unavailability of a representative dataset is the key obstacle to evaluate the techniques that capture DoH traffic in a network topology.

# Descussion
- The open source dataset has been downloaded form https://www.unb.ca/cic/datasets/dohbrw-2020.html
- which is the impact of the features to the classification performance?
     <font color='red'>As we can see in the result of the running model and As a result, classification algorithms show different performance about different kind of data structures, content and context. This implies that context-aware selection of classification algorithms will be meaningful in selecting optimal algorithms.we could develop an adaptive intelligent system which is able to automatically recognize the dy- namic change of the characteristics of data set, and then change or improve the classification algorithm in an autonomous manner. For example, data imbalance may occur from a change in contex- tual. Variable factors such as time and storage also give rise to data sets that are imbalanced. This symptom used to be called extrinsic data imbalance. In other words, classification algorithms may show different performance levels according to the context. source: Effects of data set features on the performances of classification algorithms Ohbyung Kwon a,⇑, Jae Mun Sim b,1. "I should note that increasing features always does not have a good impact on learning as shown in gaussian."</font>
- What is the feature extraction process? \
    <font color='red'>Feature extraction refers to the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set. It yields better results than applying machine learning directly to the raw data.
    Feature extraction can be accomplished manually or automatically:
    - Manual feature extraction requires identifying and describing the features that are relevant for a given problem and implementing a way to extract those features. In many situations, having a good understanding of the background or domain can help make informed decisions as to which features could be useful. Over decades of research, engineers and scientists have developed feature extraction methods for images, signals, and text. An example of a simple feature is the mean of a window in a signal.
    - Automated feature extraction uses specialized algorithms or deep networks to extract features automatically from signals or images without the need for human intervention. This technique can be very useful when you want to move quickly from raw data to developing machine learning algorithms. Wavelet scattering is an example of automated feature extraction.
    With the ascent of deep learning, feature extraction has been largely replaced by the first layers of deep networks – but mostly for image data. For signal and time-series applications, feature extraction remains the first challenge that requires significant expertise before one can build effective predictive models.</font>
- Is the Gaussian probability distribution assumption about data applicable?\
     <font color='red'>In the result of testing the dataset we can see that DecisionTreeClassifier worked better than the GaussianNB and by testing other algorithm such as KNN we can see that they will work better but we should not forget that Naive bayes (Gaussian) is one of the best distribution possiblities in machine learning and recently i hvae read an article which refered to use Gussian with SVM, and we can get better result or using other methods to improve our algorithm, such as PCA or etc.</font>
